# Copy this file to .env and adjust values as needed. Do NOT commit your real .env (contains secrets).

# Server
# HOST: bind address for FastAPI
# PORT: HTTP port to serve all stations
# TICK_HZ: authoritative simulation tick rate (20 Hz by default)
HOST=0.0.0.0
PORT=8000
TICK_HZ=20

# Persistence / Storage
# SQLITE_PATH: path to SQLite DB for runs/snapshots/events
# SNAPSHOT_S: seconds between telemetry snapshots persisted to DB
SQLITE_PATH=./sub-bridge.db
SNAPSHOT_S=2.0

# Logging
# LOG_LEVEL: DEBUG|INFO|WARNING|ERROR
LOG_LEVEL=DEBUG

# Redis (optional; currently unused by default)
# USE_REDIS: enable Redis pub/sub backend (future)
# REDIS_URL: connection string if enabled
USE_REDIS=false
REDIS_URL=redis://localhost:6379

# Gameplay / Rules
# REQUIRE_CAPTAIN_CONSENT: if true, Captain must grant consent before firing
REQUIRE_CAPTAIN_CONSENT=true

# Enemy AI (legacy simple polling stub in loop)
# USE_ENEMY_AI: enable simple periodic AI that directly adjusts RED ships (legacy)
# AI_POLL_S: poll cadence in seconds for the legacy AI stub
# ENEMY_STATIC: if true, enemy ships do not move unless AI changes orders
USE_ENEMY_AI=false
AI_POLL_S=2.0
ENEMY_STATIC=true

# Maintenance / Tasks
# FIRST_TASK_DELAY_S: initial delay before first maintenance task spawns
# MAINT_SPAWN_SCALE: multiplier on spawn interval (higher = slower spawns)
FIRST_TASK_DELAY_S=30.0
MAINT_SPAWN_SCALE=1.0

# New Agent Orchestrator (Fleet + Ship agents)
# USE_AI_ORCHESTRATOR: enables Fleet Commander + per-Ship Commander agents. Non-blocking; actions validated
# and applied on subsequent ticks. Prefer this over USE_ENEMY_AI.
USE_AI_ORCHESTRATOR=true

# Engine selection: stub|ollama|openai
# AI_FLEET_ENGINE / AI_SHIP_ENGINE determine which backend to use for each tier
# - stub: deterministic local behavior (no LLM calls)
# - ollama: local LLM via Ollama REST API
# - openai: OpenAI Agents SDK with cloud models
AI_FLEET_ENGINE=ollama
AI_SHIP_ENGINE=ollama

# Model identifiers (engine-specific)
# AI_FLEET_MODEL / AI_SHIP_MODEL
# - Ollama examples: llama3.1:8b, mistral:7b-instruct, qwen2.5:7b
# - OpenAI examples: gpt-4o-mini, o3-mini, gpt-4.1-mini
AI_FLEET_MODEL=Deepseek-r1:latest
AI_SHIP_MODEL=llama3.2:latest

# Ollama (if using AI_*_ENGINE=ollama)
# OLLAMA_HOST: base URL of your local Ollama server
# To start with Ollama locally (no cloud calls):
#   USE_AI_ORCHESTRATOR=true
#   AI_FLEET_ENGINE=ollama
#   AI_SHIP_ENGINE=ollama
#   AI_FLEET_MODEL=llama3.1:8b
#   AI_SHIP_MODEL=llama3.1:8b
OLLAMA_HOST=http://localhost:11434

# OpenAI (if using AI_*_ENGINE=openai)
# OPENAI_API_KEY: your API key (only in private .env; never commit)
# OPENAI_BASE_URL: override if using a proxy or enterprise endpoint
# To switch to OpenAI cloud models:
#   USE_AI_ORCHESTRATOR=true
#   AI_FLEET_ENGINE=openai
#   AI_SHIP_ENGINE=openai
#   AI_FLEET_MODEL=gpt-4o-mini
#   AI_SHIP_MODEL=gpt-4o-mini
OPENAI_API_KEY=your-openai-key-here
OPENAI_BASE_URL=https://api.openai.com/v1